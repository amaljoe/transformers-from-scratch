{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27538529-d57e-44c7-bd14-0eb09182f7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since xtreme couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'PAN-X.en' at /home/compiling-ganesh/24m0797/.cache/huggingface/datasets/xtreme/PAN-X.en/0.0.0/ec5f1f46e9af79639a90684a7a70a956c4998f04 (last modified on Sun Dec 14 17:32:15 2025).\n",
      "Using the latest cached version of the dataset since xtreme couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'PAN-X.eu' at /home/compiling-ganesh/24m0797/.cache/huggingface/datasets/xtreme/PAN-X.eu/0.0.0/ec5f1f46e9af79639a90684a7a70a956c4998f04 (last modified on Sun Dec 14 12:47:27 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "ds1 = load_dataset('xtreme', 'PAN-X.en')\n",
    "ds2 = load_dataset('xtreme', 'PAN-X.eu')\n",
    "\n",
    "ds = dict()\n",
    "for k in ds1.keys():\n",
    "    ds[k] = concatenate_datasets([ds1[k], ds2[k]])\n",
    "\n",
    "ds = DatasetDict(ds)\n",
    "ds = ds.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a1ea0e-8807-4ee0-8c5d-d8bc20039e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "device = 'cuda'\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = ds['train'].features['ner_tags'].feature.num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d87cd19e-1184-41ba-8b87-a7451f56c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(item):\n",
    "    res = tokenizer(item['tokens'], is_split_into_words=True, truncation=True, padding=True)\n",
    "    res['labels'] = []\n",
    "    for i in range(len(res['input_ids'])):\n",
    "        word_ids = res.word_ids(batch_index = i)\n",
    "        last_id = None\n",
    "        labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is not None and word_id != last_id:\n",
    "                last_id = word_id\n",
    "                labels.append(int(item['ner_tags'][i][word_id]))\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            last_id = word_id\n",
    "        res['labels'].append(labels)\n",
    "    return res\n",
    "\n",
    "tokenized_ds = ds.map(preprocess, batched = True)\n",
    "item = tokenized_ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d8d46e5-8fb8-4d64-9257-38a7b55514e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ellen barkin ( born 1954 )'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(item['input_ids'], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b8ae10d-ff70-47e9-bf2d-b981327ddf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ellen</td>\n",
       "      <td>Barkin</td>\n",
       "      <td>(</td>\n",
       "      <td>born</td>\n",
       "      <td>1954</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1  2     3     4  5\n",
       "0  Ellen  Barkin  (  born  1954  )\n",
       "1  B-PER   I-PER  O     O     O  O"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def pretty_print(item):\n",
    "    breakpoint()\n",
    "    text = item['tokens']\n",
    "    tags = [ds['train'].features['ner_tags'].feature.names[idx] for idx in item['labels'] if idx != -100]\n",
    "    df = pd.DataFrame([text, tags])\n",
    "    return df\n",
    "\n",
    "pretty_print(tokenized_ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23cc5f73-a300-459e-8f37-b0a74caae480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/compiling-ganesh/24m0797/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/34c46321f42186df33a6260966e34a368f14868d9cc2ba47d142112e2800d233 (last modified on Tue Nov 25 11:27:07 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorForTokenClassification, TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "f1 = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(res):\n",
    "    predictions, labels = np.argmax(res.predictions, axis=-1), res.label_ids\n",
    "    final_pred, final_labels = [], []\n",
    "    for p, l in zip(predictions, labels):\n",
    "        for pred, label in zip(p, l):\n",
    "            if label != -100:\n",
    "                final_pred.append(pred)\n",
    "                final_labels.append(label)\n",
    "    return {\n",
    "        'micro-f1': f1.compute(predictions = final_pred, references = final_labels, average = 'micro')['f1'],\n",
    "        'macro-f1': f1.compute(predictions = final_pred, references = final_labels, average = 'macro')['f1'],\n",
    "        'weighted-f1': f1.compute(predictions = final_pred, references = final_labels, average = 'weighted')['f1']\n",
    "    }\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForTokenClassification.from_pretrained(model_name, num_labels = ds['train'].features['ner_tags'].feature.num_classes).to(device)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 32,\n",
    "    output_dir = 'output/out',\n",
    "    eval_steps = 50,\n",
    "    logging_steps = 50,\n",
    "    eval_strategy = 'steps'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b512be61-1706-47fe-8133-d2b131fd811b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'wandb' has no attribute 'errors' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/__init__.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wb_logging \u001b[38;5;28;01mas\u001b[39;00m _wb_logging\n\u001b[32m     23\u001b[39m _wb_logging.configure_wandb_logger()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m     29\u001b[39m wandb.wandb_lib = wandb_sdk.lib  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/sdk/__init__.py:25\u001b[39m\n\u001b[32m      3\u001b[39m __all__ = (\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mConfig\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSettings\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhelper\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_helper \u001b[38;5;28;01mas\u001b[39;00m helper\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01martifacts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01martifact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Artifact\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwandb_alerts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlertLevel\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwandb_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/sdk/artifacts/artifact.py:44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_strutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nameof\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnormalize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_exceptions\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpublic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArtifactCollection, ArtifactFiles, Run\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpublic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gql_compat\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WBValue\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/apis/__init__.py:45\u001b[39m\n\u001b[32m     40\u001b[39m     _disable_ssl()\n\u001b[32m     43\u001b[39m reset_path = util.vendor_setup()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpublic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m PublicApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     48\u001b[39m reset_path()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/apis/internal.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mApi\u001b[39;00m:\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Internal proxy to the official internal API.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_api.py:51\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgql_request\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphQLSession\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhashutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m B64MD5, md5_file_b64\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m credentials, retry\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfilenames\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DIFF_FNAME, METADATA_FNAME\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/sdk/lib/retry.py:26\u001b[39m\n\u001b[32m     22\u001b[39m SLEEP_FN = time.sleep\n\u001b[32m     23\u001b[39m SLEEP_ASYNC_FN = asyncio.sleep\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRetryCancelledError\u001b[39;00m(\u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m.Error):\n\u001b[32m     27\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"A retry did not occur because it was cancelled.\"\"\"\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTransientError\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'wandb' has no attribute 'errors' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b1587439-f1fa-4034-8c6f-20494bd7ef88",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'wandb' has no attribute 'errors' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m      2\u001b[39m trainer = Trainer(\n\u001b[32m      3\u001b[39m     args = args,\n\u001b[32m      4\u001b[39m     model_init = model_init,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     compute_metrics = compute_metrics\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/__init__.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wb_logging \u001b[38;5;28;01mas\u001b[39;00m _wb_logging\n\u001b[32m     23\u001b[39m _wb_logging.configure_wandb_logger()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m     29\u001b[39m wandb.wandb_lib = wandb_sdk.lib  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/sdk/__init__.py:25\u001b[39m\n\u001b[32m      3\u001b[39m __all__ = (\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mConfig\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSettings\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhelper\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_helper \u001b[38;5;28;01mas\u001b[39;00m helper\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01martifacts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01martifact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Artifact\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwandb_alerts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlertLevel\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwandb_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/sdk/artifacts/artifact.py:44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_strutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nameof\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnormalize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_exceptions\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpublic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArtifactCollection, ArtifactFiles, Run\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpublic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gql_compat\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WBValue\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/apis/__init__.py:45\u001b[39m\n\u001b[32m     40\u001b[39m     _disable_ssl()\n\u001b[32m     43\u001b[39m reset_path = util.vendor_setup()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpublic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m PublicApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     48\u001b[39m reset_path()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/apis/internal.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mApi\u001b[39;00m:\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Internal proxy to the official internal API.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/sdk/internal/internal_api.py:51\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgql_request\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphQLSession\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhashutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m B64MD5, md5_file_b64\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m credentials, retry\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfilenames\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DIFF_FNAME, METADATA_FNAME\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/wandb/sdk/lib/retry.py:26\u001b[39m\n\u001b[32m     22\u001b[39m SLEEP_FN = time.sleep\n\u001b[32m     23\u001b[39m SLEEP_ASYNC_FN = asyncio.sleep\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRetryCancelledError\u001b[39;00m(\u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m.Error):\n\u001b[32m     27\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"A retry did not occur because it was cancelled.\"\"\"\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTransientError\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'wandb' has no attribute 'errors' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    args = args,\n",
    "    model_init = model_init,\n",
    "    processing_class = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = tokenized_ds['train'],\n",
    "    eval_dataset = tokenized_ds['test'].select(range(10)),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c3e2a9c-a05d-44e6-9390-1eab16299eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  4913,  5297,  ...,     0,     0,     0],\n",
       "        [  101,  1005,  1005,  ...,     0,     0,     0],\n",
       "        [  101, 11623,  2632,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  6423, 21981,  ...,     0,     0,     0],\n",
       "        [  101,  2106,  2025,  ...,     0,     0,     0],\n",
       "        [  101,  5123,  2752,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,    1,    2,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    1,    2,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100,    3,    4,  ..., -100, -100, -100],\n",
       "        [-100,    0,    0,  ..., -100, -100, -100],\n",
       "        [-100,    3,    4,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_ds[\"train\"].select_columns(['input_ids', 'labels', 'attention_mask']),\n",
    "    shuffle=True,\n",
    "    collate_fn = data_collator,\n",
    "    batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eed05f02-4530-4a02-b032-ec682b77a6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m tokenized_ds.select_columns(column_names: Union[str, list[str]]) -> \u001b[33m'DatasetDict'\u001b[39m\n",
       "\u001b[31mDocstring:\u001b[39m\n",
       "Select one or several column(s) from each split in the dataset and\n",
       "the features associated to the column(s).\n",
       "\n",
       "The transformation is applied to all the splits of the dataset\n",
       "dictionary.\n",
       "\n",
       "Args:\n",
       "    column_names (`Union[str, list[str]]`):\n",
       "        Name of the column(s) to keep.\n",
       "\n",
       "Example:\n",
       "\n",
       "```py\n",
       ">>> from datasets import load_dataset\n",
       ">>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
       ">>> ds.select_columns(\"text\")\n",
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})\n",
       "```\n",
       "\u001b[31mFile:\u001b[39m      ~/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/datasets/dataset_dict.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds.select_columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "12a4d488-a055-4fe0-a928-c0137326af53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1/3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 71.68it/s]\n",
      "  0%|          | 1/938 [00:00<05:40,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval @ step 1 | micro-f1: 0.2419 | macro-f1: 0.1370 | weighted-f1: 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 49/938 [00:14<04:32,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 | loss = 1.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 70.59it/s]\n",
      "  5%|▌         | 50/938 [00:14<05:22,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval @ step 50 | micro-f1: 0.2419 | macro-f1: 0.1370 | weighted-f1: 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 99/938 [00:29<04:26,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 | loss = 1.8931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 71.80it/s]\n",
      " 11%|█         | 100/938 [00:30<05:51,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval @ step 100 | micro-f1: 0.2419 | macro-f1: 0.1370 | weighted-f1: 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 149/938 [00:45<03:45,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150 | loss = 1.9174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 71.33it/s]\n",
      " 16%|█▌        | 150/938 [00:45<05:13,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval @ step 150 | micro-f1: 0.2419 | macro-f1: 0.1370 | weighted-f1: 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 199/938 [01:00<03:42,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 | loss = 1.9239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 71.70it/s]\n",
      " 21%|██▏       | 200/938 [01:00<04:16,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval @ step 200 | micro-f1: 0.2419 | macro-f1: 0.1370 | weighted-f1: 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 249/938 [01:15<03:05,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250 | loss = 1.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 70.86it/s]\n",
      " 27%|██▋       | 250/938 [01:16<04:26,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval @ step 250 | micro-f1: 0.2419 | macro-f1: 0.1370 | weighted-f1: 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 299/938 [01:31<03:24,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300 | loss = 1.9397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 69.58it/s]\n",
      " 32%|███▏      | 300/938 [01:31<03:52,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval @ step 300 | micro-f1: 0.2419 | macro-f1: 0.1370 | weighted-f1: 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 349/938 [01:48<03:08,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 350 | loss = 1.8487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.82it/s]\n",
      " 37%|███▋      | 350/938 [01:48<04:16,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval @ step 350 | micro-f1: 0.2419 | macro-f1: 0.1370 | weighted-f1: 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 399/938 [02:04<02:53,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400 | loss = 1.9363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 70.71it/s]\n",
      " 43%|████▎     | 400/938 [02:04<03:20,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval @ step 400 | micro-f1: 0.2419 | macro-f1: 0.1370 | weighted-f1: 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 449/938 [02:19<02:19,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450 | loss = 1.8802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 70.60it/s]\n",
      " 48%|████▊     | 450/938 [02:20<03:14,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval @ step 450 | micro-f1: 0.2419 | macro-f1: 0.1370 | weighted-f1: 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 471/938 [02:26<02:25,  3.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_dataloader)):\n\u001b[32m     47\u001b[39m     global_step += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     batch = {k: \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items()}\n\u001b[32m     51\u001b[39m     outputs = model(**batch)\n\u001b[32m     52\u001b[39m     loss = outputs.loss\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_scheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_ds[\"train\"].select_columns(['input_ids', 'labels', 'attention_mask']),\n",
    "    shuffle=True,\n",
    "    collate_fn = data_collator,\n",
    "    batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_ds[\"test\"].select(range(10)).select_columns(['input_ids', 'labels', 'attention_mask']),\n",
    "    shuffle=True,\n",
    "    collate_fn = data_collator,\n",
    "    batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    eps=args.adam_epsilon,\n",
    "    weight_decay=args.weight_decay\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "max_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.get_warmup_steps(max_steps),\n",
    "    num_training_steps=max_steps,\n",
    ")\n",
    "\n",
    "global_step = 0\n",
    "model = model_init()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(int(args.num_train_epochs)):\n",
    "    print(f\"\\n===== Epoch {epoch + 1}/{args.num_train_epochs} =====\")\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        global_step += 1\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # ---- Backprop ----\n",
    "        loss.backward()\n",
    "\n",
    "        # ---- Gradient clipping (same as Trainer) ----\n",
    "        if args.max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                args.max_grad_norm\n",
    "            )\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ---- Logging ----\n",
    "        if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "            print(f\"Step {global_step} | loss = {loss.item():.4f}\")\n",
    "\n",
    "        # ---- Evaluation ----\n",
    "        if args.eval_steps > 0 and (global_step % args.eval_steps == 0 or global_step == 1):\n",
    "            model.eval()\n",
    "\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for eval_batch in tqdm(eval_dataloader):\n",
    "                    eval_batch = {k: v.to(device) for k, v in eval_batch.items()}\n",
    "                    outputs = model(**eval_batch)\n",
    "\n",
    "                    all_preds.append(outputs.logits.cpu().numpy())\n",
    "                    all_labels.append(eval_batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "            preds = np.concatenate(all_preds, axis=0)\n",
    "            labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "            metrics = compute_metrics(\n",
    "                type(\"EvalPred\", (), {\n",
    "                    \"predictions\": preds,\n",
    "                    \"label_ids\": labels\n",
    "                })\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Eval @ step {global_step} | \"\n",
    "                + \" | \".join(f\"{k}: {v:.4f}\" for k, v in metrics.items())\n",
    "            )\n",
    "\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef420f98-88d5-4be7-8541-0da3930b233c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
