{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27538529-d57e-44c7-bd14-0eb09182f7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since xtreme couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'PAN-X.eu' at /home/compiling-ganesh/24m0797/.cache/huggingface/datasets/xtreme/PAN-X.eu/0.0.0/ec5f1f46e9af79639a90684a7a70a956c4998f04 (last modified on Sat Dec 13 20:21:00 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('xtreme', 'PAN-X.eu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a1ea0e-8807-4ee0-8c5d-d8bc20039e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "device = 'cuda'\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = ds['train'].features['ner_tags'].feature.num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d87cd19e-1184-41ba-8b87-a7451f56c6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff13737ce5834df4b4a96eee2bd94b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5174cdc1d14689a9a378162b4de900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33dd56a24c324c9396011f9c6018e3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tokens': ['BIRZUZENDU', 'Gertaera', 'arraroen', 'lege'],\n",
       " 'ner_tags': [0, 3, 4, 4],\n",
       " 'langs': ['eu', 'eu', 'eu', 'eu'],\n",
       " 'input_ids': [101,\n",
       "  12170,\n",
       "  15378,\n",
       "  20395,\n",
       "  4859,\n",
       "  2226,\n",
       "  16216,\n",
       "  13320,\n",
       "  6906,\n",
       "  12098,\n",
       "  19848,\n",
       "  8913,\n",
       "  2078,\n",
       "  4190,\n",
       "  2063,\n",
       "  102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  4,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  4,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(item):\n",
    "    res = tokenizer(item['tokens'], is_split_into_words=True, truncation=True)\n",
    "    res['labels'] = []\n",
    "    for i in range(len(res['input_ids'])):\n",
    "        word_ids = res.word_ids(batch_index = i)\n",
    "        last_id = None\n",
    "        labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is not None and word_id != last_id:\n",
    "                last_id = word_id\n",
    "                labels.append(int(item['ner_tags'][i][word_id]))\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            last_id = word_id\n",
    "        res['labels'].append(labels)\n",
    "    return res\n",
    "\n",
    "tokenized_ds = ds.map(preprocess, batched = True)\n",
    "item = tokenized_ds['train'][0]\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d8d46e5-8fb8-4d64-9257-38a7b55514e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] birzuzendu gertaera arraroen lege [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(item['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8ae10d-ff70-47e9-bf2d-b981327ddf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R.H.</td>\n",
       "      <td>Saunders</td>\n",
       "      <td>(</td>\n",
       "      <td>St.</td>\n",
       "      <td>Lawrence</td>\n",
       "      <td>River</td>\n",
       "      <td>)</td>\n",
       "      <td>(</td>\n",
       "      <td>968</td>\n",
       "      <td>MW</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1  2      3         4      5  6  7    8   9  10\n",
       "0   R.H.  Saunders  (    St.  Lawrence  River  )  (  968  MW  )\n",
       "1  B-ORG     I-ORG  O  B-ORG     I-ORG  I-ORG  O  O    O   O  O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def pretty_print(item):\n",
    "    breakpoint()\n",
    "    text = item['tokens']\n",
    "    tags = [ds['train'].features['ner_tags'].feature.names[idx] for idx in item['labels'] if idx != -100]\n",
    "    df = pd.DataFrame([text, tags])\n",
    "    return df\n",
    "\n",
    "pretty_print(tokenized_ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1587439-f1fa-4034-8c6f-20494bd7ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/compiling-ganesh/24m0797/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/34c46321f42186df33a6260966e34a368f14868d9cc2ba47d142112e2800d233 (last modified on Tue Nov 25 11:27:07 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [237/237 00:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro-f1</th>\n",
       "      <th>Macro-f1</th>\n",
       "      <th>Weighted-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.165900</td>\n",
       "      <td>0.942560</td>\n",
       "      <td>0.699355</td>\n",
       "      <td>0.317481</td>\n",
       "      <td>0.618143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.695100</td>\n",
       "      <td>0.656212</td>\n",
       "      <td>0.805161</td>\n",
       "      <td>0.521845</td>\n",
       "      <td>0.770042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.529700</td>\n",
       "      <td>0.480691</td>\n",
       "      <td>0.869677</td>\n",
       "      <td>0.708992</td>\n",
       "      <td>0.864276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.420400</td>\n",
       "      <td>0.409959</td>\n",
       "      <td>0.882581</td>\n",
       "      <td>0.767866</td>\n",
       "      <td>0.883117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.321900</td>\n",
       "      <td>0.306938</td>\n",
       "      <td>0.908387</td>\n",
       "      <td>0.812610</td>\n",
       "      <td>0.907654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.264469</td>\n",
       "      <td>0.913548</td>\n",
       "      <td>0.818285</td>\n",
       "      <td>0.912335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.258700</td>\n",
       "      <td>0.237200</td>\n",
       "      <td>0.926452</td>\n",
       "      <td>0.853028</td>\n",
       "      <td>0.926081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.238300</td>\n",
       "      <td>0.232744</td>\n",
       "      <td>0.921290</td>\n",
       "      <td>0.841133</td>\n",
       "      <td>0.921163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.235973</td>\n",
       "      <td>0.929032</td>\n",
       "      <td>0.860021</td>\n",
       "      <td>0.928879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.188300</td>\n",
       "      <td>0.209547</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.871794</td>\n",
       "      <td>0.934498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.194408</td>\n",
       "      <td>0.938065</td>\n",
       "      <td>0.873265</td>\n",
       "      <td>0.937239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.177600</td>\n",
       "      <td>0.203059</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.865801</td>\n",
       "      <td>0.934774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.166200</td>\n",
       "      <td>0.197699</td>\n",
       "      <td>0.940645</td>\n",
       "      <td>0.877259</td>\n",
       "      <td>0.939864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.195573</td>\n",
       "      <td>0.939355</td>\n",
       "      <td>0.879610</td>\n",
       "      <td>0.939228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.173598</td>\n",
       "      <td>0.941935</td>\n",
       "      <td>0.879196</td>\n",
       "      <td>0.941203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.142600</td>\n",
       "      <td>0.178605</td>\n",
       "      <td>0.944516</td>\n",
       "      <td>0.879769</td>\n",
       "      <td>0.943818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.184675</td>\n",
       "      <td>0.941935</td>\n",
       "      <td>0.873752</td>\n",
       "      <td>0.941075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.188370</td>\n",
       "      <td>0.943226</td>\n",
       "      <td>0.874836</td>\n",
       "      <td>0.942753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.118100</td>\n",
       "      <td>0.177972</td>\n",
       "      <td>0.947097</td>\n",
       "      <td>0.886169</td>\n",
       "      <td>0.946794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.177027</td>\n",
       "      <td>0.944516</td>\n",
       "      <td>0.879806</td>\n",
       "      <td>0.943858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.178990</td>\n",
       "      <td>0.948387</td>\n",
       "      <td>0.890574</td>\n",
       "      <td>0.947756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.177980</td>\n",
       "      <td>0.950968</td>\n",
       "      <td>0.894589</td>\n",
       "      <td>0.950505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.176846</td>\n",
       "      <td>0.952258</td>\n",
       "      <td>0.894135</td>\n",
       "      <td>0.951713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=237, training_loss=0.260891642751573, metrics={'train_runtime': 43.3922, 'train_samples_per_second': 691.369, 'train_steps_per_second': 5.462, 'total_flos': 2311021146378912.0, 'train_loss': 0.260891642751573, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorForTokenClassification, TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "f1 = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(res):\n",
    "    predictions, labels = np.argmax(res.predictions, axis=-1), res.label_ids\n",
    "    final_pred, final_labels = [], []\n",
    "    for p, l in zip(predictions, labels):\n",
    "        for pred, label in zip(p, l):\n",
    "            if label != -100:\n",
    "                final_pred.append(pred)\n",
    "                final_labels.append(label)\n",
    "    return {\n",
    "        'micro-f1': f1.compute(predictions = final_pred, references = final_labels, average = 'micro')['f1'],\n",
    "        'macro-f1': f1.compute(predictions = final_pred, references = final_labels, average = 'macro')['f1'],\n",
    "        'weighted-f1': f1.compute(predictions = final_pred, references = final_labels, average = 'weighted')['f1']\n",
    "    }\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForTokenClassification.from_pretrained(model_name, num_labels = ds['train'].features['ner_tags'].feature.num_classes).to(device)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 32,\n",
    "    output_dir = 'output/out',\n",
    "    eval_steps = 10,\n",
    "    logging_steps = 10,\n",
    "    eval_strategy = 'steps'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args = args,\n",
    "    model_init = model_init,\n",
    "    processing_class = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = tokenized_ds['train'],\n",
    "    eval_dataset = tokenized_ds['test'].select(range(100)),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
