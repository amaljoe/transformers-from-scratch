{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4422464c-2879-47cd-85c0-c2f2eb74b8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since xtreme couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'PAN-X.en' at /home/compiling-ganesh/24m0797/.cache/huggingface/datasets/xtreme/PAN-X.en/0.0.0/ec5f1f46e9af79639a90684a7a70a956c4998f04 (last modified on Wed Oct 15 14:09:13 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f04ea704cbb4367b1866b76241d3840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4830c2a0ed4082a44a57da31bf965d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecadab909dfd4efbb934fa451e32ffdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('xtreme', 'PAN-X.en')\n",
    "\n",
    "tags = dataset['train'].features['ner_tags'].feature\n",
    "def create_ner_tags_str(batch):\n",
    "    ner_tags_str = [tags.int2str(idx) for idx in batch['ner_tags']]\n",
    "    return {\n",
    "        'ner_tags_str': ner_tags_str,\n",
    "        'input': \n",
    "    }\n",
    "\n",
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "dataset = dataset.map(create_ner_tags_str)\n",
    "dataset, index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7067aba0-7785-4154-9b50-d0a05ef3a999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "device = 'cuda'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(tag2index.keys())).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "129acfa9-64c0-4288-90a0-cb4d8bdabb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c726b4e9f2f64a989f0c31a6fe5fa0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3650809b37643f2b7f06b4730ba35ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc28f9e0bca94aa69ea6e983309a7a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(batch):\n",
    "    tokenized_input = tokenizer(batch['tokens'], is_split_into_words=True, truncation=True)\n",
    "    batch_labels = []\n",
    "    for i in range(len(tokenized_input.input_ids)):\n",
    "        word_ids = tokenized_input.word_ids(batch_index = i)\n",
    "        labels = []\n",
    "        prev_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is not None and word_id != prev_id:\n",
    "                labels.append(batch['ner_tags'][i][word_id])\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            prev_id = word_id\n",
    "        batch_labels.append(labels)\n",
    "        \n",
    "    return {\n",
    "        'input_ids': tokenized_input['input_ids'],\n",
    "        'attention_mask': tokenized_input['attention_mask'],\n",
    "        'labels': batch_labels,\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(process, batched=True, remove_columns=['tokens', 'ner_tags', 'langs', 'ner_tags_str'])\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "06f406bb-ec0f-4fa2-999e-52f1f95a17ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([18])\n",
      "attention_mask: torch.Size([18])\n",
      "labels: torch.Size([18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1054,  1012,  1044,  1012, 15247,  1006,  2358,  1012,  5623,\n",
       "           2314,  1007,  1006,  5986,  2620, 12464,  1007,   102]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "        device='cuda:0'),\n",
       " 'labels': tensor([[-100,    3, -100, -100, -100,    4,    0,    3, -100,    4,    4,    0,\n",
       "             0,    0, -100,    0,    0, -100]], device='cuda:0')}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.set_format('pt')\n",
    "element = tokenized_dataset['train'][0]\n",
    "for k, v in element.items():\n",
    "    element[k] = v.unsqueeze(0).to(model.device)\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "\n",
    "element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0f492c21-271b-4dda-9c4d-ee0b0b7536c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 6.1471, -1.8713, -1.2632, -1.1330,  0.0871, -1.8853, -0.8286],\n",
       "         [ 0.7283,  1.5231, -1.8846,  2.5090, -1.7349,  0.3948, -2.7095],\n",
       "         [ 0.2162, -1.4112,  1.4875, -1.2552,  2.1848, -2.3096, -0.1213],\n",
       "         [-1.1195, -1.2492,  2.9788, -2.0142,  2.7425, -2.6304, -0.1973],\n",
       "         [-1.1124, -1.5560,  2.4273, -2.0839,  2.6958, -2.3937, -0.3671],\n",
       "         [-0.1287, -1.8343,  3.3499, -1.8725,  2.8535, -2.5372, -0.3614],\n",
       "         [ 3.1042, -2.2096,  0.3416, -1.7823,  2.6039, -2.7109,  0.0708],\n",
       "         [-1.2141, -1.2433, -1.5182,  2.4411,  0.6438,  1.5449, -1.3846],\n",
       "         [-0.1025, -2.7891,  0.3786, -1.3663,  3.0904, -1.8938,  1.3384],\n",
       "         [-1.7391, -2.6729,  0.6353, -1.2077,  3.1343, -1.5018,  2.3717],\n",
       "         [-0.1434, -2.1450,  0.1053, -1.5070,  3.3205, -2.0426,  2.4399],\n",
       "         [ 3.9793, -2.3165, -0.3279, -1.9936,  1.7974, -2.3550, -0.2862],\n",
       "         [ 6.0769, -1.8901, -0.7970, -1.6724,  0.7151, -2.2399, -0.8927],\n",
       "         [ 5.2583, -1.6690, -0.6087, -0.8338,  0.3321, -1.9018, -0.7822],\n",
       "         [ 4.8437, -1.7739, -0.4713, -1.0370,  0.9146, -1.6907, -0.9549],\n",
       "         [ 4.8414, -1.8946, -0.6133, -1.1538,  0.4502, -2.1961, -1.2030],\n",
       "         [ 6.2985, -1.7683, -0.7815, -1.3009,  0.0301, -1.9215, -0.8324],\n",
       "         [ 4.1033, -0.4497, -1.2087,  0.0230, -0.4639, -0.4061, -2.0894]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=element['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "84fd3b2b-6fb9-4291-923d-cbb37039eac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/compiling-ganesh/24m0797/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--f1/34c46321f42186df33a6260966e34a368f14868d9cc2ba47d142112e2800d233 (last modified on Tue Nov 25 11:27:07 2025) since it couldn't be found locally at evaluate-metric--f1, or remotely on the Hugging Face Hub.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='301' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [301/471 00:53 < 00:30, 5.57 it/s, Epoch 1.91/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-micro</th>\n",
       "      <th>F1-macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.294100</td>\n",
       "      <td>0.275568</td>\n",
       "      <td>0.859787</td>\n",
       "      <td>0.744539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.213400</td>\n",
       "      <td>0.252527</td>\n",
       "      <td>0.869519</td>\n",
       "      <td>0.753157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/79 00:04 < 00:00, 16.88 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(tag2index.keys())).to(device)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps = 10,\n",
    "    eval_steps = 100,\n",
    "    eval_strategy = 'steps',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "def compute_metrics(res):\n",
    "    batch_preds, batch_true = res.predictions, res.label_ids\n",
    "    batch_preds = np.argmax(batch_preds, axis = -1)\n",
    "    final_preds, final_true = [], []\n",
    "    for preds, true in zip(batch_preds, batch_true):\n",
    "        for p, t in zip(preds, true):\n",
    "            if t != -100 and t != 0:\n",
    "                final_preds.append(p)\n",
    "                final_true.append(t)\n",
    "        \n",
    "    return {\n",
    "        'f1-micro': f1_metric.compute(predictions=final_preds, references=final_true,\n",
    "                               average = 'micro')['f1'],\n",
    "        'f1-macro': f1_metric.compute(predictions=final_preds, references=final_true,\n",
    "                               average = 'macro')['f1']\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init = model_init,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    data_collator = data_collator,\n",
    "    processing_class = tokenizer,\n",
    "    args = args,\n",
    "    compute_metrics = compute_metrics,\n",
    "    eval_dataset = tokenized_dataset['validation']\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98aa0b41-6598-481f-9ba3-26fde8205ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>r</td>\n",
       "      <td>.</td>\n",
       "      <td>h</td>\n",
       "      <td>.</td>\n",
       "      <td>saunders</td>\n",
       "      <td>(</td>\n",
       "      <td>st</td>\n",
       "      <td>.</td>\n",
       "      <td>lawrence</td>\n",
       "      <td>river</td>\n",
       "      <td>)</td>\n",
       "      <td>(</td>\n",
       "      <td>96</td>\n",
       "      <td>##8</td>\n",
       "      <td>mw</td>\n",
       "      <td>)</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1  2  3  4         5  6   7  8         9      10 11 12  13  \\\n",
       "Tokens    [CLS]  r  .  h  .  saunders  (  st  .  lawrence  river  )  (  96   \n",
       "Word IDs   None  0  0  0  0         1  2   3  3         4      5  6  7   8   \n",
       "\n",
       "           14  15  16     17  \n",
       "Tokens    ##8  mw   )  [SEP]  \n",
       "Word IDs    8   9  10   None  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tokenized_input = tokenizer(dataset['train'][0]['tokens'], is_split_into_words=True)\n",
    "tokens = tokenized_input.tokens()\n",
    "word_ids = tokenized_input.word_ids()\n",
    "\n",
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "d585dc20-31aa-4d42-8b89-46082c4ba329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shortly</td>\n",
       "      <td>afterward</td>\n",
       "      <td>an</td>\n",
       "      <td>encouraging</td>\n",
       "      <td>response</td>\n",
       "      <td>influenced</td>\n",
       "      <td>him</td>\n",
       "      <td>to</td>\n",
       "      <td>go</td>\n",
       "      <td>to</td>\n",
       "      <td>India.</td>\n",
       "      <td>He</td>\n",
       "      <td>arrived</td>\n",
       "      <td>at</td>\n",
       "      <td>Adyar</td>\n",
       "      <td>in</td>\n",
       "      <td>1884.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1   2            3         4           5    6   7   8   9   \\\n",
       "0  Shortly  afterward  an  encouraging  response  influenced  him  to  go  to   \n",
       "1        O          O   O            O         O           O    O   O   O   O   \n",
       "\n",
       "       10  11       12  13     14  15     16  \n",
       "0  India.  He  arrived  at  Adyar  in  1884.  \n",
       "1   B-LOC   O        O   O  B-LOC   O      O  "
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def tag_text(text, model, tokenizer):\n",
    "    model.eval()\n",
    "    if isinstance(text, str):\n",
    "        text = text.split(\" \")\n",
    "    tokenized_input = tokenizer(text, is_split_into_words = True, return_tensors='pt')\n",
    "    for k, v in tokenized_input.items():\n",
    "        tokenized_input[k] = v.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        op = model(**tokenized_input)\n",
    "    logits = op.logits[0]\n",
    "    preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "    preds = [index2tag[pred] for pred in preds]\n",
    "    tokens = tokenized_input.tokens(batch_index=0)\n",
    "    word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "    text_preds = []\n",
    "    prev_word_id = None\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        if word_id is not None and word_id != prev_word_id:\n",
    "            text_preds.append(preds[i])\n",
    "        prev_word_id = word_id\n",
    "    return pd.DataFrame([text, text_preds])\n",
    "\n",
    "element = dataset['test'][0]['tokens']\n",
    "tag_text(\"Shortly afterward an encouraging response influenced him to go to India. He arrived at Adyar in 1884.\", trainer.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8f4d8843-7839-41c5-b822-1f1d5746c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(model.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
