{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4422464c-2879-47cd-85c0-c2f2eb74b8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since xtreme couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'PAN-X.en' at /home/compiling-ganesh/24m0797/.cache/huggingface/datasets/xtreme/PAN-X.en/0.0.0/ec5f1f46e9af79639a90684a7a70a956c4998f04 (last modified on Wed Oct 15 14:09:13 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f04ea704cbb4367b1866b76241d3840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4830c2a0ed4082a44a57da31bf965d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecadab909dfd4efbb934fa451e32ffdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('xtreme', 'PAN-X.en')\n",
    "\n",
    "tags = dataset['train'].features['ner_tags'].feature\n",
    "def create_ner_tags_str(batch):\n",
    "    ner_tags_str = [tags.int2str(idx) for idx in batch['ner_tags']]\n",
    "    return {\n",
    "        'ner_tags_str': ner_tags_str,\n",
    "        'input': \n",
    "    }\n",
    "\n",
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "dataset = dataset.map(create_ner_tags_str)\n",
    "dataset, index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7067aba0-7785-4154-9b50-d0a05ef3a999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "device = 'cuda'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(tag2index.keys())).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "129acfa9-64c0-4288-90a0-cb4d8bdabb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0952c54b3646c8bccbf4ef39a9f172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2baf739443814f5cba648636898971a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d386fd30074d9992a9cae404978618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(batch):\n",
    "    tokenized_input = tokenizer(batch['tokens'], is_split_into_words=True)\n",
    "    batch_labels = []\n",
    "    for i in range(len(tokenized_input.input_ids)):\n",
    "        word_ids = tokenized_input.word_ids(batch_index = i)\n",
    "        labels = []\n",
    "        prev_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is not None and word_id != prev_id:\n",
    "                labels.append(batch['ner_tags'][i][word_id])\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            prev_id = word_id\n",
    "        batch_labels.append(labels)\n",
    "        \n",
    "    return {\n",
    "        'input_ids': tokenized_input['input_ids'],\n",
    "        'attention_mask': tokenized_input['attention_mask'],\n",
    "        'labels': batch_labels,\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(process, batched=True, remove_columns=['tokens', 'ner_tags', 'langs', 'ner_tags_str'])\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "06f406bb-ec0f-4fa2-999e-52f1f95a17ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([18])\n",
      "attention_mask: torch.Size([18])\n",
      "labels: torch.Size([18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1054,  1012,  1044,  1012, 15247,  1006,  2358,  1012,  5623,\n",
       "           2314,  1007,  1006,  5986,  2620, 12464,  1007,   102]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "        device='cuda:0'),\n",
       " 'labels': tensor([[-100,    3, -100, -100, -100,    4,    0,    3, -100,    4,    4,    0,\n",
       "             0,    0, -100,    0,    0, -100]], device='cuda:0')}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.set_format('pt')\n",
    "element = tokenized_dataset['train'][0]\n",
    "for k, v in element.items():\n",
    "    element[k] = v.unsqueeze(0).to(model.device)\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "\n",
    "element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0f492c21-271b-4dda-9c4d-ee0b0b7536c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 0.1986,  0.3080, -0.6250,  0.1480, -0.3975,  0.3623, -0.0627],\n",
       "         [ 0.4878,  0.5728, -0.3992,  0.0516, -0.3633, -0.1326,  0.2375],\n",
       "         [ 0.7309,  0.3112, -0.2049, -0.0753, -0.3996, -0.1824,  0.1904],\n",
       "         [ 0.4840,  0.6467, -0.0162, -0.0218, -0.3739, -0.1098,  0.0915],\n",
       "         [ 0.5294,  0.5113, -0.2453, -0.0257, -0.2207, -0.2873, -0.0713],\n",
       "         [ 0.5163,  0.5298, -0.5072,  0.1733, -0.2610, -0.3410, -0.2983],\n",
       "         [ 0.1950,  0.0698,  0.0511, -0.2642, -0.3923, -0.1508, -0.1166],\n",
       "         [ 0.0158,  0.1740, -0.3416,  0.1781, -0.0849, -0.0343, -0.5179],\n",
       "         [ 0.4036, -0.0119, -0.0023, -0.0397,  0.0538,  0.1492, -0.3354],\n",
       "         [-0.0456,  0.3246, -0.3368,  0.1033, -0.0791, -0.0354, -0.6216],\n",
       "         [ 0.2418,  0.5791, -0.2783,  0.0538, -0.3133,  0.0307, -0.2147],\n",
       "         [ 0.2638,  0.2974, -0.1125, -0.1079, -0.1458,  0.0607,  0.0741],\n",
       "         [ 0.2353,  0.3430,  0.0530, -0.2306, -0.2994, -0.1147,  0.1270],\n",
       "         [ 0.5430,  0.3277, -0.4013, -0.0271,  0.2906,  0.0119,  0.0398],\n",
       "         [ 0.1825,  0.2841,  0.0455,  0.0216,  0.1117, -0.1033,  0.1313],\n",
       "         [ 0.2385,  0.4698,  0.1065,  0.0395, -0.7798, -0.3820,  0.0436],\n",
       "         [ 0.2994,  0.1344, -0.2239, -0.1850,  0.0297,  0.1777, -0.4396],\n",
       "         [ 0.3174,  0.0963, -0.0485, -0.0406,  0.0723,  0.0853, -0.3971]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=element['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "84fd3b2b-6fb9-4291-923d-cbb37039eac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/compiling-ganesh/24m0797/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='289' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 289/1250 00:26 < 01:28, 10.91 it/s, Epoch 0.23/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(tag2index.keys())).to(device)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size=4,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init = model_init,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    data_collator = data_collator,\n",
    "    processing_class = tokenizer,\n",
    "    args = args\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98aa0b41-6598-481f-9ba3-26fde8205ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>r</td>\n",
       "      <td>.</td>\n",
       "      <td>h</td>\n",
       "      <td>.</td>\n",
       "      <td>saunders</td>\n",
       "      <td>(</td>\n",
       "      <td>st</td>\n",
       "      <td>.</td>\n",
       "      <td>lawrence</td>\n",
       "      <td>river</td>\n",
       "      <td>)</td>\n",
       "      <td>(</td>\n",
       "      <td>96</td>\n",
       "      <td>##8</td>\n",
       "      <td>mw</td>\n",
       "      <td>)</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1  2  3  4         5  6   7  8         9      10 11 12  13  \\\n",
       "Tokens    [CLS]  r  .  h  .  saunders  (  st  .  lawrence  river  )  (  96   \n",
       "Word IDs   None  0  0  0  0         1  2   3  3         4      5  6  7   8   \n",
       "\n",
       "           14  15  16     17  \n",
       "Tokens    ##8  mw   )  [SEP]  \n",
       "Word IDs    8   9  10   None  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tokenized_input = tokenizer(dataset['train'][0]['tokens'], is_split_into_words=True)\n",
    "tokens = tokenized_input.tokens()\n",
    "word_ids = tokenized_input.word_ids()\n",
    "\n",
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d585dc20-31aa-4d42-8b89-46082c4ba329",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (1292684888.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef process(batch)\u001b[39m\n                      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
