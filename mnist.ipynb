{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178746e2-f0d6-4dd6-a487-62831aa8c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cfe513-3f5d-4bf7-bc1b-fae05c35c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52761911-2cde-4ae8-a0ad-1d81e9e4e23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since ylecun/mnist couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'mnist' at /home/compiling-ganesh/24m0797/.cache/huggingface/datasets/ylecun___mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c (last modified on Sat Nov 29 23:41:02 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ylecun/mnist\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6089974e-d406-43f1-b4b3-0d19438d4cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since ylecun/mnist couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'mnist' at /home/compiling-ganesh/24m0797/.cache/huggingface/datasets/ylecun___mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c (last modified on Sat Nov 29 23:41:02 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([10]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ds = load_dataset(\"ylecun/mnist\")\n",
    "\n",
    "def preprocess(x):\n",
    "    return {\n",
    "        'input': np.array(x['image']).reshape(-1) / 255.0,\n",
    "        'label': F.one_hot(torch.tensor(x['label']), num_classes = 10).to(torch.float32)\n",
    "    }\n",
    "\n",
    "train_ds, test_ds = ds['train'].select(range(10000)), ds['test'].select(range(100))\n",
    "\n",
    "train_ds = train_ds.map(preprocess, num_proc=64)\n",
    "test_ds = test_ds.map(preprocess, num_proc=64)\n",
    "\n",
    "train_ds.set_format('pt')\n",
    "test_ds.set_format('pt')\n",
    "\n",
    "\n",
    "train_ds[0]['input'].shape, train_ds[0]['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93aefb7c-4c9a-43ef-ac52-7a3f851273fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of param: 34114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 10/10 [00:00<00:00, 17.01it/s, loss=2.56]\n",
      "Epoch 0 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.08it/s, val_loss=2.54]\n",
      "Epoch 1: 100%|██████████| 10/10 [00:00<00:00, 17.17it/s, loss=2.37]\n",
      "Epoch 1 [val]: 100%|██████████| 1/1 [00:00<00:00, 124.10it/s, val_loss=2.37]\n",
      "Epoch 2: 100%|██████████| 10/10 [00:00<00:00, 17.16it/s, loss=2.29]\n",
      "Epoch 2 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.30it/s, val_loss=2.27]\n",
      "Epoch 3: 100%|██████████| 10/10 [00:00<00:00, 17.29it/s, loss=2.22]\n",
      "Epoch 3 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.63it/s, val_loss=2.19]\n",
      "Epoch 4: 100%|██████████| 10/10 [00:00<00:00, 17.31it/s, loss=2.17]\n",
      "Epoch 4 [val]: 100%|██████████| 1/1 [00:00<00:00, 124.90it/s, val_loss=2.12]\n",
      "Epoch 5: 100%|██████████| 10/10 [00:00<00:00, 17.27it/s, loss=2.12]\n",
      "Epoch 5 [val]: 100%|██████████| 1/1 [00:00<00:00, 129.08it/s, val_loss=2.06]\n",
      "Epoch 6: 100%|██████████| 10/10 [00:00<00:00, 17.18it/s, loss=2.07]\n",
      "Epoch 6 [val]: 100%|██████████| 1/1 [00:00<00:00, 126.51it/s, val_loss=2]\n",
      "Epoch 7: 100%|██████████| 10/10 [00:00<00:00, 17.36it/s, loss=2.03]\n",
      "Epoch 7 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.87it/s, val_loss=1.96]\n",
      "Epoch 8: 100%|██████████| 10/10 [00:00<00:00, 17.34it/s, loss=1.99]\n",
      "Epoch 8 [val]: 100%|██████████| 1/1 [00:00<00:00, 131.09it/s, val_loss=1.92]\n",
      "Epoch 9: 100%|██████████| 10/10 [00:00<00:00, 17.31it/s, loss=1.95]\n",
      "Epoch 9 [val]: 100%|██████████| 1/1 [00:00<00:00, 130.11it/s, val_loss=1.89]\n",
      "Epoch 10: 100%|██████████| 10/10 [00:00<00:00, 17.31it/s, loss=1.92]\n",
      "Epoch 10 [val]: 100%|██████████| 1/1 [00:00<00:00, 129.64it/s, val_loss=1.87]\n",
      "Epoch 11: 100%|██████████| 10/10 [00:00<00:00, 11.52it/s, loss=1.89]\n",
      "Epoch 11 [val]: 100%|██████████| 1/1 [00:00<00:00, 129.86it/s, val_loss=1.84]\n",
      "Epoch 12: 100%|██████████| 10/10 [00:00<00:00, 17.31it/s, loss=1.86]\n",
      "Epoch 12 [val]: 100%|██████████| 1/1 [00:00<00:00, 131.34it/s, val_loss=1.82]\n",
      "Epoch 13: 100%|██████████| 10/10 [00:00<00:00, 17.43it/s, loss=1.83]\n",
      "Epoch 13 [val]: 100%|██████████| 1/1 [00:00<00:00, 129.11it/s, val_loss=1.79]\n",
      "Epoch 14: 100%|██████████| 10/10 [00:00<00:00, 17.42it/s, loss=1.8]\n",
      "Epoch 14 [val]: 100%|██████████| 1/1 [00:00<00:00, 131.16it/s, val_loss=1.77]\n",
      "Epoch 15: 100%|██████████| 10/10 [00:00<00:00, 17.35it/s, loss=1.77]\n",
      "Epoch 15 [val]: 100%|██████████| 1/1 [00:00<00:00, 130.31it/s, val_loss=1.74]\n",
      "Epoch 16: 100%|██████████| 10/10 [00:00<00:00, 17.38it/s, loss=1.74]\n",
      "Epoch 16 [val]: 100%|██████████| 1/1 [00:00<00:00, 129.47it/s, val_loss=1.72]\n",
      "Epoch 17: 100%|██████████| 10/10 [00:00<00:00, 17.29it/s, loss=1.71]\n",
      "Epoch 17 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.39it/s, val_loss=1.7]\n",
      "Epoch 18: 100%|██████████| 10/10 [00:00<00:00, 17.30it/s, loss=1.69]\n",
      "Epoch 18 [val]: 100%|██████████| 1/1 [00:00<00:00, 126.44it/s, val_loss=1.67]\n",
      "Epoch 19: 100%|██████████| 10/10 [00:00<00:00, 17.13it/s, loss=1.66]\n",
      "Epoch 19 [val]: 100%|██████████| 1/1 [00:00<00:00, 126.37it/s, val_loss=1.64]\n",
      "Epoch 20: 100%|██████████| 10/10 [00:00<00:00, 17.33it/s, loss=1.63]\n",
      "Epoch 20 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.29it/s, val_loss=1.61]\n",
      "Epoch 21: 100%|██████████| 10/10 [00:00<00:00, 17.34it/s, loss=1.6]\n",
      "Epoch 21 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.30it/s, val_loss=1.58]\n",
      "Epoch 22: 100%|██████████| 10/10 [00:00<00:00, 17.22it/s, loss=1.56]\n",
      "Epoch 22 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.96it/s, val_loss=1.54]\n",
      "Epoch 23: 100%|██████████| 10/10 [00:00<00:00, 17.27it/s, loss=1.53]\n",
      "Epoch 23 [val]: 100%|██████████| 1/1 [00:00<00:00, 127.42it/s, val_loss=1.51]\n",
      "Epoch 24: 100%|██████████| 10/10 [00:00<00:00, 11.43it/s, loss=1.49]\n",
      "Epoch 24 [val]: 100%|██████████| 1/1 [00:00<00:00, 126.03it/s, val_loss=1.48]\n",
      "Epoch 25: 100%|██████████| 10/10 [00:00<00:00, 17.33it/s, loss=1.44]\n",
      "Epoch 25 [val]: 100%|██████████| 1/1 [00:00<00:00, 123.29it/s, val_loss=1.45]\n",
      "Epoch 26: 100%|██████████| 10/10 [00:00<00:00, 17.31it/s, loss=1.4]\n",
      "Epoch 26 [val]: 100%|██████████| 1/1 [00:00<00:00, 124.03it/s, val_loss=1.42]\n",
      "Epoch 27: 100%|██████████| 10/10 [00:00<00:00, 17.21it/s, loss=1.35]\n",
      "Epoch 27 [val]: 100%|██████████| 1/1 [00:00<00:00, 130.38it/s, val_loss=1.39]\n",
      "Epoch 28: 100%|██████████| 10/10 [00:00<00:00, 17.36it/s, loss=1.31]\n",
      "Epoch 28 [val]: 100%|██████████| 1/1 [00:00<00:00, 129.53it/s, val_loss=1.37]\n",
      "Epoch 29: 100%|██████████| 10/10 [00:00<00:00, 17.38it/s, loss=1.28]\n",
      "Epoch 29 [val]: 100%|██████████| 1/1 [00:00<00:00, 123.16it/s, val_loss=1.33]\n",
      "Epoch 30: 100%|██████████| 10/10 [00:00<00:00, 17.36it/s, loss=1.24]\n",
      "Epoch 30 [val]: 100%|██████████| 1/1 [00:00<00:00, 124.35it/s, val_loss=1.3]\n",
      "Epoch 31: 100%|██████████| 10/10 [00:00<00:00, 17.30it/s, loss=1.21]\n",
      "Epoch 31 [val]: 100%|██████████| 1/1 [00:00<00:00, 130.31it/s, val_loss=1.27]\n",
      "Epoch 32: 100%|██████████| 10/10 [00:00<00:00, 17.35it/s, loss=1.18]\n",
      "Epoch 32 [val]: 100%|██████████| 1/1 [00:00<00:00, 129.50it/s, val_loss=1.24]\n",
      "Epoch 33: 100%|██████████| 10/10 [00:00<00:00, 17.24it/s, loss=1.14]\n",
      "Epoch 33 [val]: 100%|██████████| 1/1 [00:00<00:00, 124.46it/s, val_loss=1.21]\n",
      "Epoch 34: 100%|██████████| 10/10 [00:00<00:00, 17.32it/s, loss=1.11]\n",
      "Epoch 34 [val]: 100%|██████████| 1/1 [00:00<00:00, 129.35it/s, val_loss=1.19]\n",
      "Epoch 35: 100%|██████████| 10/10 [00:00<00:00, 17.15it/s, loss=1.08]\n",
      "Epoch 35 [val]: 100%|██████████| 1/1 [00:00<00:00, 129.29it/s, val_loss=1.16]\n",
      "Epoch 36: 100%|██████████| 10/10 [00:00<00:00, 17.35it/s, loss=1.05]\n",
      "Epoch 36 [val]: 100%|██████████| 1/1 [00:00<00:00, 130.40it/s, val_loss=1.13]\n",
      "Epoch 37: 100%|██████████| 10/10 [00:00<00:00, 11.50it/s, loss=1.03]\n",
      "Epoch 37 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.73it/s, val_loss=1.1]\n",
      "Epoch 38: 100%|██████████| 10/10 [00:00<00:00, 17.24it/s, loss=1]   \n",
      "Epoch 38 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.40it/s, val_loss=1.08]\n",
      "Epoch 39: 100%|██████████| 10/10 [00:00<00:00, 17.31it/s, loss=0.974]\n",
      "Epoch 39 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.86it/s, val_loss=1.05]\n",
      "Epoch 40: 100%|██████████| 10/10 [00:00<00:00, 17.21it/s, loss=0.95]\n",
      "Epoch 40 [val]: 100%|██████████| 1/1 [00:00<00:00, 130.36it/s, val_loss=1.03]\n",
      "Epoch 41: 100%|██████████| 10/10 [00:00<00:00, 17.31it/s, loss=0.927]\n",
      "Epoch 41 [val]: 100%|██████████| 1/1 [00:00<00:00, 122.59it/s, val_loss=1.01]\n",
      "Epoch 42: 100%|██████████| 10/10 [00:00<00:00, 17.28it/s, loss=0.902]\n",
      "Epoch 42 [val]: 100%|██████████| 1/1 [00:00<00:00, 125.53it/s, val_loss=0.986]\n",
      "Epoch 43: 100%|██████████| 10/10 [00:00<00:00, 17.28it/s, loss=0.88]\n",
      "Epoch 43 [val]: 100%|██████████| 1/1 [00:00<00:00, 130.28it/s, val_loss=0.965]\n",
      "Epoch 44: 100%|██████████| 10/10 [00:00<00:00, 17.33it/s, loss=0.86]\n",
      "Epoch 44 [val]: 100%|██████████| 1/1 [00:00<00:00, 129.30it/s, val_loss=0.943]\n",
      "Epoch 45: 100%|██████████| 10/10 [00:00<00:00, 17.32it/s, loss=0.839]\n",
      "Epoch 45 [val]: 100%|██████████| 1/1 [00:00<00:00, 130.40it/s, val_loss=0.923]\n",
      "Epoch 46: 100%|██████████| 10/10 [00:00<00:00, 17.34it/s, loss=0.819]\n",
      "Epoch 46 [val]: 100%|██████████| 1/1 [00:00<00:00, 130.49it/s, val_loss=0.904]\n",
      "Epoch 47: 100%|██████████| 10/10 [00:00<00:00, 17.37it/s, loss=0.8] \n",
      "Epoch 47 [val]: 100%|██████████| 1/1 [00:00<00:00, 130.00it/s, val_loss=0.886]\n",
      "Epoch 48: 100%|██████████| 10/10 [00:00<00:00, 17.37it/s, loss=0.781]\n",
      "Epoch 48 [val]: 100%|██████████| 1/1 [00:00<00:00, 128.75it/s, val_loss=0.868]\n",
      "Epoch 49: 100%|██████████| 10/10 [00:00<00:00, 17.33it/s, loss=0.763]\n",
      "Epoch 49 [val]: 100%|██████████| 1/1 [00:00<00:00, 124.69it/s, val_loss=0.851]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DigitNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.relu(self.fc1(x))\n",
    "        a2 = self.relu(self.fc2(a1))\n",
    "        logits = self.fc3(a2)\n",
    "        return logits\n",
    "\n",
    "class DigitTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers, d, num_heads, num_classes):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.seq_len = input_dim // self.d\n",
    "        self.encoder = Encoder(num_layers=num_layers, d=d, num_heads=num_heads)\n",
    "        self.head = nn.Linear(d, num_classes)\n",
    "        self.pos_emb = nn.Embedding(self.seq_len, self.d)\n",
    "                         \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.seq_len, self.d)\n",
    "        x = x + self.pos_emb(torch.arange(self.seq_len, device=x.device))\n",
    "        enc_out = self.encoder(x)\n",
    "        last_hidden_state = enc_out[:, -1, :]\n",
    "        logits = self.head(last_hidden_state)\n",
    "        return logits\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d, num_heads):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.layers = nn.ModuleList([EncoderBlock(d, num_heads) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d, num_heads, batch_first=True)\n",
    "        self.fc1 = nn.Linear(d, d)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_q = nn.Linear(d, d)\n",
    "        self.fc_k = nn.Linear(d, d)\n",
    "        self.fc_v = nn.Linear(d, d)\n",
    "        self.ln1 = nn.LayerNorm(d)\n",
    "        self.ln2 = nn.LayerNorm(d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        q, k , v = self.fc_q(x), self.fc_k(x), self.fc_v(x)\n",
    "        a1, wts = self.mha(q, k, v)\n",
    "        a1 = self.ln2(a1)\n",
    "        a1 = self.relu(self.fc1(a1))\n",
    "        return x + a1\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "train_dataloader = DataLoader(train_ds, batch_size=1024)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=1024)\n",
    "# model = DigitNet(28*28, 2048, 10)\n",
    "model = DigitTransformer(input_dim = 28 * 28, num_layers=5, d=28, num_heads=7, num_classes=10)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(lr=1e-4, params = model.parameters())\n",
    "num_param = 0\n",
    "for param in model.parameters():\n",
    "    num_param += torch.numel(param)\n",
    "print(\"Num of param:\", num_param)\n",
    "\n",
    "\n",
    "for epoch in range(50):\n",
    "    loop = tqdm(train_dataloader, desc=f\"Epoch {epoch}\")\n",
    "    for batch in loop:\n",
    "        ip, labels = batch['input'], batch['label']\n",
    "        ip, labels = ip.to(device), labels.to(device)\n",
    "        logits = model(ip)\n",
    "        loss = criterion(logits, labels)\n",
    "        loop.set_postfix({'loss': loss.item()})\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    # ---- VALIDATION ----\n",
    "    val_loop = tqdm(test_dataloader, desc=f\"Epoch {epoch} [val]\")\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loop:\n",
    "            ip, labels = batch['input'], batch['label']\n",
    "            ip, labels = ip.to(device), labels.to(device)\n",
    "            logits = model(ip)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loop.set_postfix({'val_loss': loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b23d92be-0337-428e-a182-c0aa59b374fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f7a9e91-3024-4bd3-9adf-6ef49f7e5cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7265,  0.7010],\n",
       "         [ 2.0223,  0.5510],\n",
       "         [-0.5087, -0.2149]],\n",
       "\n",
       "        [[ 0.7265,  0.7010],\n",
       "         [ 2.0223,  0.5510],\n",
       "         [-0.5087, -0.2149]],\n",
       "\n",
       "        [[ 0.7265,  0.7010],\n",
       "         [ 2.0223,  0.5510],\n",
       "         [-0.5087, -0.2149]],\n",
       "\n",
       "        [[ 0.7265,  0.7010],\n",
       "         [ 2.0223,  0.5510],\n",
       "         [-0.5087, -0.2149]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(3, 2)\n",
    "x = torch.ones((4,3,2))\n",
    "x + emb(torch.arange(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff24c4f3-a2ff-4595-8bd7-93aa1a5334ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of param: 5824522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 625/625 [00:18<00:00, 33.72it/s, loss=0.14]  \n",
      "Epoch 1:  69%|██████▉   | 430/625 [00:14<00:06, 30.34it/s, loss=0.619] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m loop.set_postfix({\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: loss.item()})\n\u001b[32m     19\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/torch/optim/adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/torch/optim/adam.py:537\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    535\u001b[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m     \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch.is_complex(params[i]):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=16)\n",
    "model = DigitNet(28*28, 2048, 10)\n",
    "# model = DigitTransformer(input_dim = 28 * 28, num_layers=2, d=28 * 28, num_heads=4, num_classes=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(lr=1e-4, params = model.parameters())\n",
    "num_param = 0\n",
    "for param in model.parameters():\n",
    "    num_param += torch.numel(param)\n",
    "print(\"Num of param:\", num_param)\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    loop = tqdm(train_dataloader, desc=f\"Epoch {epoch}\")\n",
    "    for batch in loop:\n",
    "        ip, labels = batch['input'], batch['label']\n",
    "        logits = model(ip)\n",
    "        loss = criterion(logits, labels)\n",
    "        loop.set_postfix({'loss': loss.item()})\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5692451-3860-4643-a10e-c8a52ba22f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7, device='cuda:0') tensor(7, device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(pred, label)\n\u001b[32m     10\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m12\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/matplotlib/pyplot.py:3601\u001b[39m, in \u001b[36mimshow\u001b[39m\u001b[34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[39m\n\u001b[32m   3579\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.imshow)\n\u001b[32m   3580\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimshow\u001b[39m(\n\u001b[32m   3581\u001b[39m     X: ArrayLike | PIL.Image.Image,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3599\u001b[39m     **kwargs,\n\u001b[32m   3600\u001b[39m ) -> AxesImage:\n\u001b[32m-> \u001b[39m\u001b[32m3601\u001b[39m     __ret = \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3605\u001b[39m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[43m=\u001b[49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3606\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3607\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3611\u001b[39m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3613\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3617\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3618\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3619\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3620\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3621\u001b[39m     sci(__ret)\n\u001b[32m   3622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/matplotlib/__init__.py:1524\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1521\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1523\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1524\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1530\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1531\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/matplotlib/axes/_axes.py:5982\u001b[39m, in \u001b[36mAxes.imshow\u001b[39m\u001b[34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[39m\n\u001b[32m   5979\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5980\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_aspect(aspect)\n\u001b[32m-> \u001b[39m\u001b[32m5982\u001b[39m \u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5983\u001b[39m im.set_alpha(alpha)\n\u001b[32m   5984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im.get_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5985\u001b[39m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/matplotlib/image.py:685\u001b[39m, in \u001b[36m_ImageBase.set_data\u001b[39m\u001b[34m(self, A)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL.Image.Image):\n\u001b[32m    684\u001b[39m     A = pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28mself\u001b[39m._A = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28mself\u001b[39m._imcache = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/matplotlib/image.py:646\u001b[39m, in \u001b[36m_ImageBase._normalize_image_array\u001b[39m\u001b[34m(A)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_normalize_image_array\u001b[39m(A):\n\u001b[32m    642\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[33;03m    Check validity of image-like input *A* and normalize it to a format suitable for\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[33;03m    Image subclasses.\u001b[39;00m\n\u001b[32m    645\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m     A = \u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_masked_invalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m A.dtype != np.uint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.can_cast(A.dtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msame_kind\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    648\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage data of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconverted to float\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/matplotlib/cbook.py:684\u001b[39m, in \u001b[36msafe_masked_invalid\u001b[39m\u001b[34m(x, copy)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_masked_invalid\u001b[39m(x, copy=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m     x = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x.dtype.isnative:\n\u001b[32m    686\u001b[39m         \u001b[38;5;66;03m# If we have already made a copy, do the byteswap in place, else make a\u001b[39;00m\n\u001b[32m    687\u001b[39m         \u001b[38;5;66;03m# copy with the byte order swapped.\u001b[39;00m\n\u001b[32m    688\u001b[39m         \u001b[38;5;66;03m# Swap to native order.\u001b[39;00m\n\u001b[32m    689\u001b[39m         x = x.byteswap(inplace=copy).view(x.dtype.newbyteorder(\u001b[33m'\u001b[39m\u001b[33mN\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/transformers-from-scratch/.venv/lib/python3.13/site-packages/torch/_tensor.py:1211\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1209\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for x in test_ds.select(range(3)):\n",
    "    ip, label = x['input'], x['label']\n",
    "    ip, label = ip.to(device), label.to(device)\n",
    "    logits = model(ip)\n",
    "    pred = torch.argmax(logits)\n",
    "    label = torch.argmax(label)\n",
    "    img = ip.reshape(28, 28) * 255\n",
    "    print(pred, label)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
